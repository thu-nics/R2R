# æ™ºèƒ½è‡ªåŠ¨åŒ–ä»»åŠ¡è°ƒåº¦å™¨ä½¿ç”¨æŒ‡å—

`auto_scheduler.py` æ˜¯ä¸€ä¸ªå®Œå…¨è‡ªåŠ¨åŒ–çš„ LLM continuation ä»»åŠ¡è°ƒåº¦å™¨ï¼Œä¸“ä¸ºç®€åŒ–å¤§è§„æ¨¡æ•°æ®å¤„ç†è€Œè®¾è®¡ã€‚

## ğŸš€ ä¸»è¦ç‰¹æ€§

### 1. **è‡ªåŠ¨GPUæ£€æµ‹ä¸åˆ†é…**
- è‡ªåŠ¨æ£€æµ‹å½“å‰å¯ç”¨çš„ GPU åŠå…¶æ˜¾å­˜çŠ¶å†µ
- æ™ºèƒ½åˆ†é… GPU å¯¹ç»™ä¸åŒçš„ worker
- æ”¯æŒåŠ¨æ€ GPU èµ„æºç®¡ç†

### 2. **æ™ºèƒ½ä»»åŠ¡æ¢å¤**
- é€šè¿‡ worker å‚æ•°æ–‡ä»¶ï¼ˆ`worker_args_*.json`ï¼‰è·Ÿè¸ªä»»åŠ¡çŠ¶æ€
- è‡ªåŠ¨æ£€æµ‹å·²å¯åŠ¨ã€å·²å®Œæˆæˆ–å¤±è´¥çš„ä»»åŠ¡
- è·³è¿‡å·²å®Œæˆçš„ workerï¼Œé‡è¯•å¤±è´¥çš„ä»»åŠ¡
- æ”¯æŒä¸­æ–­åç²¾ç¡®ç»§ç»­æ‰§è¡Œ

### 3. **è‡ªåŠ¨åˆå¹¶ç»“æœ**
- æ‰€æœ‰ worker å®Œæˆåè‡ªåŠ¨åˆå¹¶ç»“æœæ–‡ä»¶
- ç”Ÿæˆè¯¦ç»†çš„æ‰§è¡ŒæŠ¥å‘Šå’Œç»Ÿè®¡ä¿¡æ¯
- ä¿è¯æ•°æ®å®Œæ•´æ€§å’Œä¸€è‡´æ€§

### 4. **å®æ—¶ç›‘æ§**
- å®æ—¶æ˜¾ç¤ºä»»åŠ¡è¿›åº¦å’ŒçŠ¶æ€
- è‡ªåŠ¨å¤„ç†å¤±è´¥çš„ä»»åŠ¡
- æ”¯æŒä¼˜é›…çš„ä¸­æ–­å’Œæ¸…ç†

### 5. **è¯¦ç»†è¾“å‡ºæ§åˆ¶**
- å¯åŠ¨å‰æ˜¾ç¤ºæ¯ä¸ª worker çš„å®Œæ•´æ‰§è¡Œå‘½ä»¤
- å®æ—¶æ˜¾ç¤ºæ¯ä¸ª worker çš„è¾“å‡ºå†…å®¹
- æ”¯æŒé™é»˜æ¨¡å¼ï¼Œéšè— worker è¾“å‡º
- å¤šçº¿ç¨‹å®‰å…¨çš„è¾“å‡ºæ˜¾ç¤º

### 6. **å®‰å…¨è¿›ç¨‹ç®¡ç†**
- åªæ¸…ç†ç”±å½“å‰è°ƒåº¦å™¨å®ä¾‹å¯åŠ¨çš„è¿›ç¨‹
- é€šè¿‡å”¯ä¸€è°ƒåº¦å™¨ ID è·Ÿè¸ªè¿›ç¨‹å½’å±
- é¿å…è¯¯ç»ˆæ­¢å…¶ä»–è°ƒåº¦å™¨æˆ–ç³»ç»Ÿè¿›ç¨‹
- ä¼˜é›…ç»ˆæ­¢ä¸å¼ºåˆ¶ç»ˆæ­¢çš„åˆ†çº§å¤„ç†

## ğŸ“‹ ç³»ç»Ÿè¦æ±‚

### å¿…éœ€ä¾èµ–
```bash
pip install pandas numpy torch
```

### å¯é€‰ä¾èµ–ï¼ˆå¼ºçƒˆæ¨èï¼‰
```bash
pip install pynvml  # ç”¨äºGPUæ˜¾å­˜æ£€æµ‹
```

å¦‚æœæ²¡æœ‰å®‰è£… `pynvml`ï¼Œç³»ç»Ÿå°†ä½¿ç”¨é»˜è®¤çš„ GPU æ£€æµ‹æ–¹æ¡ˆã€‚

## ğŸ”§ åŸºæœ¬ä½¿ç”¨

### æœ€ç®€å•çš„ä½¿ç”¨æ–¹å¼
```bash
python script/data_labeling/auto_scheduler.py \
    --input_path output/playground/prediction_comparison.csv \
    --output_path output/playground/continuation_auto \
    --num_workers 4
```

### å®Œæ•´å‚æ•°ç¤ºä¾‹
```bash
python script/data_labeling/auto_scheduler.py \
    --input_path output/playground/prediction_comparison.csv \
    --output_path output/playground/continuation_auto \
    --num_workers 8 \
    --gpus_per_worker 2 \
    --min_gpu_memory 12.0 \
    --max_parallel_workers 4 \
    --batch_size 32 \
    --max_new_tokens 8192 \
    --temperature 0.0 \
    --show_worker_output
```

### é™é»˜æ¨¡å¼
```bash
# é™é»˜æ¨¡å¼ - ä¸æ˜¾ç¤ºworkerçš„è¯¦ç»†è¾“å‡º
python script/data_labeling/auto_scheduler.py \
    --input_path output/playground/prediction_comparison.csv \
    --output_path output/playground/continuation_auto \
    --num_workers 4 \
    --quiet
```

## ğŸ“ å‚æ•°è¯´æ˜

### æ ¸å¿ƒå‚æ•°
- `--input_path`: è¾“å…¥ CSV æ–‡ä»¶è·¯å¾„ï¼ˆstep 1 çš„è¾“å‡ºï¼‰
- `--output_path`: è¾“å‡ºç›®å½•è·¯å¾„
- `--num_workers`: æ€» worker æ•°é‡ï¼ˆå¿…éœ€ï¼‰

### GPU é…ç½®
- `--gpus_per_worker`: æ¯ä¸ª worker ä½¿ç”¨çš„ GPU æ•°é‡ï¼ˆé»˜è®¤ï¼š2ï¼‰
- `--min_gpu_memory`: æœ€å° GPU ç©ºé—²æ˜¾å­˜è¦æ±‚ï¼Œå•ä½ GBï¼ˆé»˜è®¤ï¼š10.0ï¼‰
- `--max_parallel_workers`: æœ€å¤§å¹¶è¡Œ worker æ•°é‡ï¼ˆé»˜è®¤ï¼š8ï¼‰

### æ¨¡å‹å‚æ•°
- `--tp_size`: å¼ é‡å¹¶è¡Œå¤§å°ï¼ˆé»˜è®¤ï¼š2ï¼‰
- `--dp_size`: æ•°æ®å¹¶è¡Œå¤§å°ï¼ˆé»˜è®¤ï¼š1ï¼‰
- `--batch_size`: æ‰¹å¤„ç†å¤§å°ï¼ˆé»˜è®¤ï¼š32ï¼‰
- `--max_new_tokens`: æœ€å¤§ç”Ÿæˆ token æ•°ï¼ˆé»˜è®¤ï¼š8192ï¼‰
- `--temperature`: é‡‡æ ·æ¸©åº¦ï¼ˆé»˜è®¤ï¼š0.0ï¼‰

### è¾“å‡ºæ§åˆ¶
- `--show_worker_output`: æ˜¾ç¤º worker çš„è¯¦ç»†è¾“å‡ºå’Œæ‰§è¡Œå‘½ä»¤ï¼ˆé»˜è®¤ï¼šå¼€å¯ï¼‰
- `--quiet`: é™é»˜æ¨¡å¼ï¼Œéšè—æ‰€æœ‰ worker è¾“å‡ºï¼ˆä¸ `--show_worker_output` ç›¸åï¼‰

å®Œæ•´å‚æ•°åˆ—è¡¨å¯é€šè¿‡ `python auto_scheduler.py --help` æŸ¥çœ‹ã€‚

## ğŸ¯ å·¥ä½œæµç¨‹

### 1. åˆå§‹åŒ–é˜¶æ®µ
```
[INFO] === Starting Automatic LLM Continuation Scheduler ===
[INFO] Loading input data from output/playground/prediction_comparison.csv
[INFO] Found 1000 unique data samples (IDs: 0 to 999)
```

### 2. ä»»åŠ¡åˆ†é…é˜¶æ®µ
```
[INFO] Task range: data_id [0, 250) (250 samples)
[INFO] Task range: data_id [250, 500) (250 samples)
[INFO] Task range: data_id [500, 750) (250 samples)
[INFO] Task range: data_id [750, 1000) (250 samples)
```

### 3. GPU æ£€æµ‹é˜¶æ®µ
```
[INFO] Available GPUs: [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] Allocated GPU pairs: [[0, 1], [2, 3], [4, 5], [6, 7]]
```

### 4. ä»»åŠ¡æ‰§è¡Œé˜¶æ®µ
```
[INFO] Found 2 pending tasks out of 4 total
[INFO] Starting worker for range [500, 750) on GPUs [4, 5]
================================================================================
Worker Command for range [500, 750):
CUDA_VISIBLE_DEVICES=4,5 \
python script/data_labeling/step_2_llm_continuation.py \
    --input_path output/playground/prediction_comparison.csv \
    --output_path output/playground/continuation_auto \
    --low 500 \
    --high 750 \
    --tp_size 2 \
    --dp_size 1 \
    --batch_size 32 \
    --max_new_tokens 8192 \
    --temperature 0.0
================================================================================
[INFO] Worker [500, 750) output:
------------------------------------------------------------
[Worker 500-750] Loading input data...
[Worker 500-750] Processing batch 1/10...
[Worker 500-750] Generated continuations for 32 mismatches
[Worker 500-750] Saved results to output/continuation_auto/generation_results_data_500_to_750_real.csv
------------------------------------------------------------
[INFO] Worker [500, 750) completed successfully
[INFO] Progress: 2/4 completed, 2 running, elapsed: 180.5s
```

### 5. ç»“æœåˆå¹¶é˜¶æ®µ
```
[INFO] Starting result merge...
[INFO] Loaded 156 rows from generation_results_data_0_to_250_real.csv
[INFO] Loaded 143 rows from generation_results_data_250_to_500_real.csv
[INFO] Merged 4 files into generation_results_data_all_real_merged.csv
[INFO] Total rows in merged file: 612
```

## ğŸ“ è¾“å‡ºæ–‡ä»¶ç»“æ„

æ‰§è¡Œå®Œæˆåï¼Œoutput ç›®å½•å°†åŒ…å«ï¼š

```
output/playground/continuation_auto/
â”œâ”€â”€ generation_results_data_0_to_250_real.csv      # å•ä¸ªworkerç»“æœ
â”œâ”€â”€ generation_results_data_250_to_500_real.csv
â”œâ”€â”€ generation_results_data_500_to_750_real.csv
â”œâ”€â”€ generation_results_data_750_to_1000_real.csv
â”œâ”€â”€ generation_results_data_all_real_merged.csv    # åˆå¹¶åçš„æœ€ç»ˆç»“æœ
â”œâ”€â”€ worker_args_0_250.json                         # Workeræ‰§è¡Œå‚æ•°å’ŒçŠ¶æ€
â”œâ”€â”€ worker_args_250_500.json
â”œâ”€â”€ worker_args_500_750.json
â”œâ”€â”€ worker_args_750_1000.json
â”œâ”€â”€ scheduler_config.json                          # è°ƒåº¦å™¨é…ç½®
â”œâ”€â”€ merge_summary.json                            # åˆå¹¶ç»Ÿè®¡ä¿¡æ¯
â””â”€â”€ args.json                                     # å…¨å±€æ‰§è¡Œå‚æ•°
```

## ğŸ”„ æ–­ç‚¹ç»­ä¼ 

è°ƒåº¦å™¨æ”¯æŒæ™ºèƒ½æ–­ç‚¹ç»­ä¼ ï¼š

1. **è‡ªåŠ¨æ£€æµ‹å·²å®Œæˆä»»åŠ¡**ï¼šé‡æ–°è¿è¡Œæ—¶è‡ªåŠ¨æ‰«æ output ç›®å½•
2. **è·³è¿‡å®Œæˆçš„ worker**ï¼šåªè¿è¡Œæœªå®Œæˆæˆ–å¤±è´¥çš„ä»»åŠ¡
3. **ä¿æŒæ•°æ®ä¸€è‡´æ€§**ï¼šç¡®ä¿ä¸ä¼šé‡å¤å¤„ç†ç›¸åŒçš„æ•°æ®

ç¤ºä¾‹ï¼š
```bash
# ç¬¬ä¸€æ¬¡è¿è¡Œï¼ˆå¯èƒ½ä¸­æ–­ï¼‰
python auto_scheduler.py --input_path data.csv --output_path output --num_workers 8

# é‡æ–°è¿è¡Œï¼ˆè‡ªåŠ¨è·³è¿‡å·²å®Œæˆçš„ä»»åŠ¡ï¼‰
python auto_scheduler.py --input_path data.csv --output_path output --num_workers 8
```

## âš¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. GPU é…ç½®
- ç¡®ä¿æ¯ä¸ª GPU æœ‰è¶³å¤Ÿçš„æ˜¾å­˜ï¼ˆæ¨è â‰¥ 12GBï¼‰
- æ ¹æ® GPU æ•°é‡åˆç†è®¾ç½® `--num_workers`
- ä½¿ç”¨ `--max_parallel_workers` æ§åˆ¶å¹¶å‘åº¦

### 2. æ‰¹å¤„ç†ä¼˜åŒ–
- å¢å¤§ `--batch_size` æé«˜ GPU åˆ©ç”¨ç‡
- æ ¹æ®æ˜¾å­˜å¤§å°è°ƒæ•´ `--max_new_tokens`

### 3. ç›‘æ§èµ„æº
```bash
# ç›‘æ§ GPU ä½¿ç”¨æƒ…å†µ
nvidia-smi -l 1

# ç›‘æ§ç³»ç»Ÿèµ„æº
htop
```

## ğŸ› ï¸ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**Q: "No available GPU pairs found!"**
A: æ£€æŸ¥ GPU çŠ¶æ€å’Œæ˜¾å­˜ä½¿ç”¨æƒ…å†µï¼Œé™ä½ `--min_gpu_memory` å‚æ•°

**Q: "Worker failed for range [X, Y)"**
A: æ£€æŸ¥è¾“å…¥æ•°æ®æ ¼å¼ï¼ŒæŸ¥çœ‹é”™è¯¯æ—¥å¿—ï¼Œç¡®è®¤æ¨¡å‹è·¯å¾„æ­£ç¡®

**Q: åˆå¹¶ç»“æœæ–‡ä»¶ä¸ºç©º**
A: æ£€æŸ¥å•ä¸ª worker çš„è¾“å‡ºæ–‡ä»¶ï¼Œç¡®è®¤ä»»åŠ¡æ­£ç¡®å®Œæˆ

**Q: "TypeError: Object of type int64 is not JSON serializable"**
A: å·²è‡ªåŠ¨å¤„ç†numpy/pandasç±»å‹è½¬æ¢ï¼Œæ— éœ€æ‰‹åŠ¨å¤„ç†ã€‚å¦‚ä»æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ•°æ®æ ¼å¼

**Q: æ‹…å¿ƒè¿›ç¨‹æ¸…ç†ä¼šå½±å“å…¶ä»–ä»»åŠ¡**
A: è°ƒåº¦å™¨ä½¿ç”¨å”¯ä¸€ ID è·Ÿè¸ªè¿›ç¨‹ï¼Œåªä¼šæ¸…ç†è‡ªå·±å¯åŠ¨çš„ workerï¼Œä¸ä¼šå½±å“å…¶ä»–è°ƒåº¦å™¨æˆ–ç³»ç»Ÿè¿›ç¨‹

### è°ƒè¯•æ¨¡å¼
```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
export PYTHONPATH=/path/to/your/project
python auto_scheduler.py --is_print --num_workers 1 [å…¶ä»–å‚æ•°...]
```

## ğŸ“Š æ€§èƒ½åŸºå‡†

å…¸å‹æ€§èƒ½è¡¨ç°ï¼ˆ8 GPU æœåŠ¡å™¨ï¼‰ï¼š

| Workeræ•°é‡ | GPUé…ç½® | å¹¶è¡Œåº¦ | å¤„ç†é€Ÿåº¦ | æ˜¾å­˜ä½¿ç”¨ |
|-----------|---------|-------|----------|----------|
| 4         | 2 GPU/worker | 4     | ~100 samples/min | ~20GB/worker |
| 6         | 2 GPU/worker | 4     | ~150 samples/min | ~20GB/worker |
| 8         | 1 GPU/worker | 8     | ~200 samples/min | ~12GB/worker |

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [åŸå§‹è„šæœ¬ç”Ÿæˆå™¨](./generate_job_scripts.py) - æ‰‹åŠ¨è„šæœ¬ç”Ÿæˆæ–¹æ¡ˆ
- [åˆ†å¸ƒå¼æ‰§è¡Œå™¨](./launch_llm_continuation_multi_node.py) - Ray åˆ†å¸ƒå¼æ–¹æ¡ˆ
- [Step 2 è¯¦ç»†è¯´æ˜](./step_2_llm_continuation.py) - æ ¸å¿ƒå¤„ç†é€»è¾‘

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š
1. è¾“å…¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®
2. GPU æ˜¾å­˜æ˜¯å¦å……è¶³
3. ä¾èµ–åŒ…æ˜¯å¦å®Œæ•´å®‰è£…
4. æ—¥å¿—æ–‡ä»¶ä¸­çš„è¯¦ç»†é”™è¯¯ä¿¡æ¯ 